{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82607697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image: torch.Size([1, 3, 256, 256])\n",
      "output size of model: torch.Size([1, 4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "#%% define model\n",
    "################################ define the model ResUnet#####################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "            ###self.resblock= ResBlock(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bRV1w6eCQFVVW3Q9RZeanm2bC9hjAT7d\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "# New Residule Block    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels))\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.down_sample = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsample(x)\n",
    "        out = self.double_conv(x)\n",
    "        out = self.relu(out + identity)\n",
    "        return out\n",
    "\n",
    "class ResUNet(nn.Module):\n",
    "    \"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(ResUNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.res1= ResBlock(64,64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.res2= ResBlock(128, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.res3= ResBlock(256, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.res4= ResBlock(512, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        res1= self.res1(x1) \n",
    "        # print(\"1st conv block\", x1.shape)\n",
    "        # print(\"1st res block\", res1.shape)\n",
    "        x2 = self.down1(x1)\n",
    "        res2= self.res2(x2)\n",
    "        # print(\"sec conv block\", x2.shape)\n",
    "        # print(\"sec res block\", res2.shape)\n",
    "        x3 = self.down2(x2)\n",
    "        res3= self.res3(x3)\n",
    "        # print(\"3rd conv block\", x3.shape)\n",
    "        # print(\"3rd res block\", res3.shape)\n",
    "        x4 = self.down3(x3)\n",
    "        res4= self.res4(x4)\n",
    "        # print(\"4 conv block\", x4.shape)\n",
    "        # print(\"4 res block\", res4.shape)\n",
    "        x5 = self.down4(x4)\n",
    "        #print(\"Base down \", x5.shape)\n",
    "        x = self.up1(x5, res4)\n",
    "        #print(\"1st up block\", x.shape)\n",
    "        x = self.up2(x, res3)\n",
    "        #print(\" sec up block\", x.shape)\n",
    "        x = self.up3(x, res2)\n",
    "        #print(\"3rd up block\", x.shape)\n",
    "        x = self.up4(x, res1)\n",
    "   \n",
    "        logits = self.outc(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# generate random input (batch size, channel, height, width)\n",
    "inp=torch.rand(1,3,256,256)\n",
    "print(\"test image:\",inp.shape)\n",
    "    \n",
    "# Giving Classes & Channels\n",
    "n_classes=4\n",
    "n_channels=3\n",
    "# #\n",
    "# ##Creating model Class \n",
    "model =ResUNet(n_channels, n_classes)\n",
    "#\n",
    "## Giving random input (inp) to the model\n",
    "out=model(inp)\n",
    "#\n",
    "print(\"output size of model:\",out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e376309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% denslyUnet model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                        growth_rate, kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "        \n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "class up_in(nn.Sequential):\n",
    "    def __init__(self, num_input_features1, num_input_features2, num_output_features):\n",
    "        super(up_in, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.add_module('conv1_1', nn.Conv2d(num_input_features1, num_input_features2,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('conv3_3', nn.Conv2d(num_input_features2, num_output_features,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_output_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "    def forward(self, x,y):\n",
    "        y = self.up(y)\n",
    "        x = self.conv1_1(x)\n",
    "        z = self.conv3_3(x+y)\n",
    "        z = self.norm(z)\n",
    "        z = self.relu(z)\n",
    "        return z\n",
    "\n",
    "class upblock(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(upblock, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.add_module('conv3_3', nn.Conv2d(num_input_features, num_output_features,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_output_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "    def forward(self, x,y):\n",
    "        y = self.up(y)\n",
    "        z = self.conv3_3(x+y)\n",
    "        z = self.norm(z)\n",
    "        z = self.relu(z)\n",
    "        return z\n",
    "\n",
    "class up_out(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(up_out, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.add_module('conv3_3', nn.Conv2d(num_input_features, num_output_features,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False))\n",
    "        self.dropout = nn.Dropout2d(p=0.3)\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_output_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "    def forward(self, y):\n",
    "        y = self.up(y)\n",
    "        y = self.conv3_3(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.norm(y)\n",
    "        y = self.relu(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class DenseUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels, num_classes,growth_rate=48, block_config=(6, 12, 36, 24),\n",
    "                num_init_features=96, bn_size=4, drop_rate=0,):\n",
    "\n",
    "        super(DenseUNet, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(num_channels, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        self.up1 = up_in(48*44, 48*46, 48*16)\n",
    "        self.up2 = upblock(48*16, 48*8)\n",
    "        self.up3 = upblock(48*8, 96)\n",
    "        self.up4 = upblock(96,96)\n",
    "        self.up5 = up_out(96,64)\n",
    "        self.outconv = outconv(64,num_classes)\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features.conv0(x)\n",
    "        x0 = self.features.norm0(features)\n",
    "        x0 = self.features.relu0(x0)\n",
    "        x1 = self.features.pool0(x0)\n",
    "        x1 = self.features.denseblock1(x1)\n",
    "        x2 = self.features.transition1(x1)\n",
    "        x2 = self.features.denseblock2(x2)\n",
    "        x3 = self.features.transition2(x2)\n",
    "        x3 = self.features.denseblock3(x3)\n",
    "        x4 = self.features.transition3(x3)\n",
    "        x4 = self.features.denseblock4(x4)\n",
    "        \n",
    "        y4 = self.up1(x3, x4)\n",
    "        y3 = self.up2(x2, y4)\n",
    "        y2 = self.up3(x1, y3)\n",
    "        \n",
    "        y1 = self.up4(x0, y2)\n",
    "        y0 = self.up5(y1)\n",
    "        out = self.outconv(y0)\n",
    "        # out = F.softmax(out, dim=1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "model=DenseUNet(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563b77f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image: torch.Size([1, 3, 256, 256])\n",
      "output size of model: torch.Size([1, 4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# generate random input (batch size, channel, height, width)\n",
    "inp=torch.rand(1,3,256,256)\n",
    "print(\"test image:\",inp.shape)\n",
    "    \n",
    "# Giving Classes & Channels\n",
    "n_classes=4\n",
    "n_channels=3\n",
    "## Giving random input (inp) to the model\n",
    "out=model(inp)\n",
    "#\n",
    "print(\"output size of model:\",out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e26c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
